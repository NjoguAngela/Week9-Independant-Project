# -*- coding: utf-8 -*-
"""Titanic_Dataset_Analysis-Machine_Learning-IP_WK9_Angela_Njogu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13RwqvAB1lm1Favo_6VMxLH6KVgMw8IC3

## Reading the data

Importing relevant libraries:
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing, metrics, neighbors, model_selection

"""Loading of our data:"""

train =pd.read_csv("/content/train_cleaned.csv")
test  =pd.read_csv("/content/test_cleaned.csv")

"""Setting parameters for my visualizations"""

plt.rcParams['figure.figsize'] = (10,8)
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['axes.titleweight'] = 700
sns.set_style('darkgrid')

"""## Exploratory Data Analysis

The EDA will only be performed on the training dataset

### Univariate analysis
"""

# plotting the numbers of survivors and casualties
sns.countplot(x=train['Survived'], palette='cubehelix')
plt.title("Plot of survivors and non-survivors")

"""- There were more casualties than survivors. The margin between the two categories is not very large."""

# plotting the number of people who embarked from the three ports
sns.countplot(x=train['Embarked'], palette='cubehelix')
plt.title('Number of people embarked from each location')

"""- Most of the passengers began their journey at southhampton and the least began from the port of Queenstown."""

# plotting the number of passengers in the different classes
sns.countplot(x= train['Pclass'], palette='cubehelix')
plt.title("Number of passengers in the different classes")

"""- 3rd class had the largest number of passengers and 2nd class the least."""

# plotting the distribution of the passengers' total number of spouses or siblings

sns.distplot(train['SibSp'], bins=5)
plt.title("Distribution of number of spouses or siblings")

"""- A majority of the passengers had 0 or 1 spouse or sibling only."""

# plotting the ditribution of number of parents/children and individual is travelling with

sns.distplot(train['Parch'], bins=5)
plt.title("Distribution of number of parents and children")

"""- A majority of passengers had 0 or at least 1 parent or child.

### Bivariate analysis
"""

# plotting the number of survivors and casualties as per the ports they embarked from.

sns.countplot(x= train['Embarked'], hue= train['Survived'], palette='cubehelix')
plt.title("Number of casualties and survivors per port.")

"""- Cherboug is the only category that has more survivors than casualties."""

# plotting the fare distribution for the different classes
sns.boxplot(x= train['Pclass'], y= train['Fare'], palette='cubehelix')
plt.title("Fare for passengers in the first, second and third class ")

"""- As expected the prices are higher for first class tickets than all the others and 3rd class the least. 
- The 1st class column has a very conspicuous outlier.
"""

# plotting the age distributions of survivors and casualties

sns.boxplot(x= train['Survived'], y=train['Age'], palette='cubehelix')
plt.title("Comparison of casualties' and survivors' age distributions.")

"""- The age mean of the casualties is slightly higher than that of the survivors."""

# plotting the number of male and female casualties and survivors

sns.countplot(x= train['Sex'], hue= train['Survived'], palette='cubehelix')

"""- A majority of the female population in the ship survived the shipwreck while a majority of the men did not."""

# plotting the mean number of parents or children for survivors and casualties

sns.barplot(x= train['Survived'], y= train['Parch'], palette='cubehelix')

"""- The average number of parents/children accompanying the individual on the trip was higher for survivors."""

# plotting the number of siblings or spouses an individual had on the journey against whether
# they survived or not 

sns.barplot(x= train['Survived'], y= train['SibSp'], palette='cubehelix')

"""- The average number of spouses/siblings was higher for the casualties

### Multivariate analysis
"""

# plotting the variable correlations
sns.heatmap(train.corr())

"""Number of siblings/spouses and number of parents/children seem to be the only columns that are related to each other.

## K- Nearest Neighbours

#### Encoding my categorical data columns

- These columns include:

    1. Sex
    2. Embarked
"""

def encode(data,col):

  df = pd.get_dummies(data[col], drop_first=True)
  return df

sex =encode(train,'Sex')

embarked =encode(train,'Embarked')

# conatenating the three dataframes, the dummy dataframes and the original then dropping the 
# embarked and sex columns.
Train = pd.concat([train, embarked, sex], axis=1)
Train.drop(['Embarked','Sex'], axis=1, inplace=True)

# previewing the encoded dataframe
Train.head()

Train.dtypes

"""#### Standardizing my data"""

# calling my standard scaler method

sc = preprocessing.StandardScaler()

# splitting my data into training and test sets
y= Train['Survived'].values
X= Train.drop(['Survived'], axis=1).values

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=98)

# fitting and transforming the x_train and transforming x_test

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Performing LDA to maximize the separability of the categories. Larger distances between data points of different categories ensures higher accuracy during n_neighbours classification"""

# Instatiating LDA

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

LDA = LinearDiscriminantAnalysis()

X_train = LDA.fit_transform(X_train, y_train)
X_test = LDA.transform(X_test)

"""#### Building my KNN model"""

# instatiating my knn classifier
knn = neighbors.KNeighborsClassifier()

# training the data
model = knn.fit(X_train, y_train)

# generating predicted values
pred = model.predict(X_test)

#computing the accuracy score
model.score(X_test, y_test)

"""- The accuracy of the model is 69.87%"""

metrics.confusion_matrix(y_test, pred)

"""#### Optimizing my KNN model"""

# creating a dataframe with the parameter values to be plugged into the knn estimator
param_dict = {
    'n_neighbors': [1,3,5,7,9,11,13,14,15,17,19,21,23,25,27,29],
    'metric': ['minkowski', 'eucledian', 'manhattan']
}

# instatiate GridSearchCV

grid = model_selection.GridSearchCV(estimator= knn, param_grid=param_dict, n_jobs=-1, cv=10)

grid.fit(X_train, y_train)

# displaying the best parameters for my model.
grid.best_params_

# instatiating the new model, training data using the model and computing
# the accuracy score

new_model =grid.best_estimator_

new_model =new_model.fit(X_train, y_train)


new_model.score(X_test, y_test)

"""- The new score is better than the prior KNN model. I t acheived an accuracy of 75.64%."""

# plotting a graph for accuracy of the model across the different n_neighbour values

train_scores= []
test_scores= []
neighbours = list(np.arange(1,30,1))

# creating a for loop that computes the accuracy of the model as the n_neighbours parameter value changes.
for k in neighbours:

  KNN = neighbors.KNeighborsClassifier(n_neighbors=k)
  KNN = KNN.fit(X_train, y_train)

  acc_train = KNN.score(X_train, y_train)
  acc_test = KNN.score(X_test, y_test)

  train_scores.append(acc_train)
  test_scores.append(acc_test)

plt.plot(neighbours,train_scores, label='train accuracy')
plt.plot(neighbours,test_scores, label='test accuracy')
plt.title("Plot of testing and training accuracy across different vakues for n_neighbours")
plt.legend()
plt.show()

"""- The tesing accuracy appear to peak about 13 or 14 n_neighbours."""

# changing the size of the splits to observe changes in the performance of the model if any.

bx_train, bx_test, by_train, by_test = model_selection.train_test_split(X,y, test_size=.3, random_state=98)

split_ano =grid.best_estimator_.fit(bx_train, by_train)
split_ano.score(bx_test, by_test)

"""- The accuracy is lower using the 70:30 split."""

new_pred= split_ano.predict(bx_test)
metrics.confusion_matrix(by_test, new_pred)

ax_train, ax_test, ay_train, ay_test = model_selection.train_test_split(X,y, test_size=.4, random_state=98)

split_ano =grid.best_estimator_.fit(ax_train, ay_train)
print(split_ano.score(ax_test, ay_test))


newer_pred= split_ano.predict(ax_test)
metrics.confusion_matrix(ay_test, newer_pred)

"""- The accuracy is even lower using the 60:40 split than the 70:30 split.

Predicting the categories in the test dataframe:
"""

# encoding the columns in test

tst_sex =encode(test, 'Sex')
tst_loc =encode(test, 'Embarked')

Test = pd.concat([test, tst_sex,tst_loc], axis=1)
Test.drop(['Embarked','Sex'], axis=1,inplace=True)
Test

# dropping duplicates
Test.drop_duplicates(inplace=True)

# dropping null values
Test.dropna(inplace=True)

# standardizing our data and perfroming LDA
std_test = sc.transform(Test)
test = LDA.transform(Test)

predicted_values =new_model.predict(Test)
predicted_values =pd.DataFrame(predicted_values)
predicted_values.value_counts()

"""- The records in the test dataframe have 266 of them being classified as casualties and 113 being classified as survivors."""